{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be cleaning, parsing, and counting the words in a text file, so we'll want to use regular expressions and the \"count vectorizer\" library from scikit-learn.  To deal with the tab delimited file and numerical operations on what can become large arrays and matrices, we'll also import pandas and numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll break down a series of reviews into a list of their N most frequent terms, also called a \"bag of words\".  We'll start with the trivial case of two records and two categories, a good movie review and a bad movie review.\n",
    "\n",
    "This file contains three column headers, a record ID, a \"category\", and some unstructured text.  The category, marked 0 or 1, indicates whether a review was negative (0) or positive (1).  \n",
    "\n",
    "Note that we're loading this into a pandas dataframe.  There's only one record in the sampleGood.tsv file but we'll benefit from pandas operations when we generalize this to include a large number of files in a training set.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "good_review = pd.read_csv('data/samplePositive.tsv', header=0, delimiter=\"\\t\", quoting=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the text for a fictitious positive movie review.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Wow what a good movie.  Absolutely excellent, so good.  I loved it.  Music, dancing, action, intrigue, really good.  I\\'d highly recommend this excellent film.\"'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_review['text'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's a clearly negative movie review.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bad_review = pd.read_csv('data/sampleNegative.tsv', header=0, delimiter=\"\\t\", quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"What a terrible, terrible film.  Truly bad.  In fact, I might call it awful.  I might have to call it awful twice.  The music score was dreary, the acting was contrived, the plot was not believable or convincing.  Avoid. Terrible.\"  '"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_review['text'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We now have two categories, \"good\" and \"bad\", and two reviews, one for each category. \n",
    "\n",
    "With this minimal set of two reviews, one representing each category, we can now create a \"bag of words\", a list of the most frequent terms that show up in all movie reviews . \n",
    "\n",
    "To generate this \"bag of words\", we'll first generate a new list that holds all the reviews in the set (in this case, one positive review, one negative review).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_reviews = []\n",
    "all_reviews.append(good_review['text'][0])\n",
    "all_reviews.append(bad_review['text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"Wow what a good movie.  Absolutely excellent, so good.  I loved it.  Music, dancing, action, intrigue, really good.  I\\'d highly recommend this excellent film.\"', '\"What a terrible, terrible film.  Truly bad.  In fact, I might call it awful.  I might have to call it awful twice.  The music score was dreary, the acting was contrived, the plot was not believable or convincing.  Avoid. Terrible.\"  ']\n"
     ]
    }
   ],
   "source": [
    "print(all_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to break this list into its most common terms.  \n",
    "\n",
    "Python's scikit-learn library has a method, CountVectorizer, for this task.  It accepts a list of strings (in our case, movie reviews), and returns a list of the N most common terms.  If no number is supplied, CountVectorizer will simply return all the words that appear in the reviews, which is fine for now since our data set is very small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer = \"word\")\n",
    "bag_of_words = vectorizer.fit(all_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "And voila, We now have a \"bag of words\" for our movie reviews!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['absolutely', 'acting', 'action', 'avoid', 'awful', 'bad', 'believable', 'call', 'contrived', 'convincing', 'dancing', 'dreary', 'excellent', 'fact', 'film', 'good', 'have', 'highly', 'in', 'intrigue', 'it', 'loved', 'might', 'movie', 'music', 'not', 'or', 'plot', 'really', 'recommend', 'score', 'so', 'terrible', 'the', 'this', 'to', 'truly', 'twice', 'was', 'what', 'wow']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the vectorizer provides feature names in alphabetical order.  To get the numerical position of each term, you can use the vocabulary_ property.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'wow': 40, 'what': 39, 'good': 15, 'movie': 23, 'absolutely': 0, 'excellent': 12, 'so': 31, 'loved': 21, 'it': 20, 'music': 24, 'dancing': 10, 'action': 2, 'intrigue': 19, 'really': 28, 'highly': 17, 'recommend': 29, 'this': 34, 'film': 14, 'terrible': 32, 'truly': 36, 'bad': 5, 'in': 18, 'fact': 13, 'might': 22, 'call': 7, 'awful': 4, 'have': 16, 'to': 35, 'twice': 37, 'the': 33, 'score': 30, 'was': 38, 'dreary': 11, 'acting': 1, 'contrived': 8, 'plot': 27, 'not': 25, 'believable': 6, 'or': 26, 'convincing': 9, 'avoid': 3}\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a bag of words, we can calculate the number of times these words appear in each review.  The resulting data structure is often called a \"word vector\", an index of the frequency at which each word appears.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we can also supply \"stop words\".  These are frequently occurring terms that often have no meaning and can clutter up an algorithm (note - in real world applications, you may discover that things you thought were devoid of meaning actually make a difference in context!)  \n",
    "\n",
    "The scikit-learn library provides methods to remove stop words from a bag of words.  Here, we use the stop_words parameter to remove the common english stop words (\"a, all, and, also...\").  We'll also limit the number of terms to the most frequent ten words through the max_features parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['awful', 'excellent', 'film', 'good', 'music', 'really', 'recommend', 'score', 'terrible', 'truly']\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(analyzer = \"word\", stop_words = 'english', max_features = 10)\n",
    "bag_of_words = vectorizer.fit(all_reviews)\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Vectors\n",
    "\n",
    "Now that we have a list of the most common words in all reviews, we count the frequency with which these words appear in each review.  Each review can be decomposed into a word count vector, a list of how often each word in the most frequent terms appears in a particular review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "good_review_vector = vectorizer.transform([good_review['text'][0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is an array with a word count corresponding to each term in the bag of words.  As you can see below, in our \"good\" review, the word \"awful\" (first index) doesn't appear, whereas \"excellent\" (second index) shows up twice.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 2, 1, 3, 1, 1, 1, 0, 0, 0]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_review_vector.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can take a look at the word count for a bad review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bad_review_vector = vectorizer.transform([bad_review['text'][0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, awful shows up twice, and excellent is missing.  Note that some of the terms, such as film or music, show up once in both the good and bad review.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 0, 1, 0, 1, 0, 0, 1, 3, 1]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_review_vector.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be helpful to look at the three side-by-side.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['awful', 'excellent', 'film', 'good', 'music', 'really', 'recommend', 'score', 'terrible', 'truly']\n",
      "[[0 2 1 3 1 1 1 0 0 0]\n",
      " [2 0 1 0 1 0 0 1 3 1]]\n"
     ]
    }
   ],
   "source": [
    "all_review_vector = vectorizer.transform(all_reviews)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(all_review_vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine Learning\n",
    "\n",
    "So far, this has all been data carpentry.  We find the most common words in a set of documents, and we create a word vector to represent each individual document in the set.\n",
    "\n",
    "Now that we have this data, we can use it to train a computer to recognize positive and negative reviews based on patterns it finds in the word vectors associated with positive and negative reviews.  That is the essence of supervised machine learning.  We have a set of records assigned to a pre-defined set of categories, and we use it to train a computer to find a way to categorize records into those categories.  \n",
    "\n",
    "Although we haven't discussed creating a method to categorize reviews into \"positive\" and \"negative\", you may already be thinking of some strategies based on what you've seen here.  By creating a list, or \"bag of words\" of common terms, and decomposing each review into a word count vector, we can create a signature for each review.  We can then associate these vectors with each category and come up with some kind of rule for matching a word count with a positive or negative review.  \n",
    "\n",
    "Before continuing, you might try designing and implementing an approach yourself.  You have a positive and a negative review, and a single word vector for each.  How could you use this to program a computer to predict whether a new review is positive or negative?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Real Data\n",
    "\n",
    "We'll use a genuine, real world data set of positive and negative movie reviews, available at http://www.cs.cornell.edu/home/llee/papers/sentiment.home.html\n",
    "\n",
    "Keep in mind, even this has been pre-precessed a bit for us.  Data carpentry is a huge part of data science, and getting data into a form where you can use it plays a huge role in an application of machine learning.  Finding, cleaning, munging, formatting, and preparing data isn't just a technical task.  You almost always have to make some decisions about what to keep, what to discard, and how to prepare it, and these decisions often introduce patterns and assumptions that influence the outcome when you apply a ML technique.   \n",
    "\n",
    "Even in case of well managed and prepared data, as we have with the sentient data from Cornell, I did have to do some formatting to get the data set into a tab delimited format that can be easily imported into pandas.  The scripts to do this are located in this repository in rawdata/review_polarity/text_sentiment/createfile.py (another script to randomize the order of the reviews is available in shufflefile.py).  These files aren't especially elaborate, but it does go to show, you'll most likely have to do some processing filtering even under the very best circumstances (and most of your raw data won't be anywhere near as well presented as the data download for the sentient data here).  Also, this data set is small, only 500 positive and negative movie reviews.  Cleaning and parsing a very large data set is an entirely different challenge!\n",
    "\n",
    "With that said, let's create a bag of words and word vector for the sentiment data set.  \n",
    "\n",
    "Bag of Words\n",
    "\n",
    "Although our example above was limited to two reviews and two categories, we'll the same approach at a larger scale. \n",
    "\n",
    "The file trainRecords.csv is a tab delimited file with has 500 reviews, split into positive and negative reviews.  The tabs correspond to our sample files above - the first column is an identifier, the second is a category (0 for negative, 1 for positive), and the third contains the text of the review.  We'll use this data, long with the vectorizer method from scikit-learn, to build a bag of words, with a maximum of 5000 terms.  \n",
    "\n",
    "First, we'll read in all the reviews from a tab delimied file.  Each row contains an ID, a category (1 represents a positive review, 0 a negative one), and the text of the movie review. \n",
    "\n",
    "First, let's load the file into a pandas dataframe, as before.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/trainReviews.tsv', header=0, delimiter=\"\\t\", quoting=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create a list of reviews.  Note that we're using a regular expression to remove all text that is not alphanumeric, so we don't have to deal with punctuation or tags.  This keeps it simple, but keep in mind, those tags and other characters may actualy have meaning sometimes.  You may actaully lose context or other information when you do this.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_records = []\n",
    "for i in range( 0, len(train[\"text\"])):\n",
    "    text = train[\"text\"][i]\n",
    "    text = re.sub(\"[^a-zA-Z0-9]\",\" \", text)\n",
    "    train_records.append(text.lower())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the list of text to find a bag of words for all reviews, positive and negative.  We'll limit the bag of words to 200 terms so we can inspect it more easily.  For a real application, we'd probabably want set set this value to a much higher number.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = 'english',   \\\n",
    "                             max_features = 200)\n",
    "\n",
    "\n",
    "train_data_features = vectorizer.fit_transform(train_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now that we've created a bag of wrods, we can list the 200 most frequent terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10', 'acting', 'action', 'actor', 'actors', 'actually', 'audience', 'away', 'bad', 'based', 'begins', 'best', 'better', 'big', 'bit', 'black', 'called', 'cast', 'character', 'characters', 'city', 'come', 'comedy', 'comes', 'comic', 'completely', 'couple', 'course', 'david', 'day', 'dead', 'death', 'despite', 'dialogue', 'did', 'didn', 'different', 'director', 'does', 'doesn', 'don', 'effects', 'end', 'ending', 'especially', 'evil', 'example', 'fact', 'family', 'far', 'father', 'feel', 'film', 'films', 'final', 'friend', 'fun', 'funny', 'game', 'gets', 'getting', 'given', 'gives', 'goes', 'going', 'good', 'got', 'great', 'group', 'guy', 'half', 'hand', 'hard', 'having', 'head', 'help', 'high', 'hollywood', 'home', 'horror', 'hour', 'human', 'humor', 'idea', 'instead', 'interesting', 'isn', 'jackie', 'james', 'job', 'john', 'just', 'kind', 'know', 'left', 'let', 'life', 'like', 'line', 'little', 'll', 'long', 'look', 'looking', 'looks', 'lost', 'lot', 'love', 'main', 'make', 'makes', 'making', 'man', 'men', 'michael', 'mind', 'minutes', 'moments', 'money', 'mother', 'movie', 'movies', 'mr', 'music', 'new', 'night', 'old', 'original', 'people', 'performance', 'performances', 'picture', 'place', 'planet', 'play', 'played', 'plays', 'plot', 'point', 'pretty', 'probably', 'quite', 'real', 'really', 'reason', 'rest', 'right', 'role', 'run', 'say', 'scene', 'scenes', 'school', 'screen', 'script', 'second', 'seen', 'sense', 'series', 'set', 'sex', 'shows', 'simply', 'small', 'soon', 'special', 'star', 'stars', 'story', 'sure', 'takes', 'thing', 'things', 'think', 'thought', 'time', 'times', 'town', 'true', 'trying', 'turns', 'unfortunately', 'use', 've', 'want', 'wants', 'watch', 'watching', 'way', 'wife', 'woman', 'won', 'work', 'works', 'world', 'written', 'wrong', 'year', 'years', 'young']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the most common 200 terms, we can decompose each review in the set into a word vector, counting the frequency of each term in a review. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'data/fullNegativeReview.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-882b3400fec8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnegative_review\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/fullNegativeReview.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquoting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/boushey/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    644\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 646\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/boushey/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mchunksize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/boushey/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/boushey/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    921\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 923\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    924\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/boushey/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1388\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1390\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1392\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader.__cinit__ (pandas/parser.c:4184)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._setup_parser_source (pandas/parser.c:8449)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'data/fullNegativeReview.csv' does not exist"
     ]
    }
   ],
   "source": [
    "negative_review = pd.read_csv('data/negativeReview.csv', header=0, delimiter=\"\\t\", quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'negative' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-ce6847a914bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnegative\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'negative' is not defined"
     ]
    }
   ],
   "source": [
    "negative[\"text\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can decompose this review into a word count vector, showing the frequency of each term in the top 200 terms for the full set of reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bad_review_data_features = vectorizer.transform([full_bad_review[\"text\"][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 2, 0, 2, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "       1, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_review_data_features.toarray()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a little hard to track, so let's go ahead and print the term and frequency side by side for this review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 0\n",
      "acting 0\n",
      "action 0\n",
      "actor 0\n",
      "actors 1\n",
      "actually 0\n",
      "audience 1\n",
      "away 0\n",
      "bad 1\n",
      "based 0\n",
      "begins 0\n",
      "best 1\n",
      "better 0\n",
      "big 0\n",
      "bit 0\n",
      "black 0\n",
      "called 0\n",
      "cast 0\n",
      "character 2\n",
      "characters 2\n",
      "city 0\n",
      "come 2\n",
      "comedy 0\n",
      "comes 0\n",
      "comic 0\n",
      "completely 0\n",
      "couple 0\n",
      "course 0\n",
      "david 0\n",
      "day 1\n",
      "dead 0\n",
      "death 0\n",
      "despite 0\n",
      "dialogue 1\n",
      "did 0\n",
      "didn 1\n",
      "different 0\n",
      "director 1\n",
      "does 1\n",
      "doesn 0\n",
      "don 1\n",
      "effects 0\n",
      "end 0\n",
      "ending 0\n",
      "especially 0\n",
      "evil 0\n",
      "example 0\n",
      "fact 0\n",
      "family 0\n",
      "far 0\n",
      "father 0\n",
      "feel 0\n",
      "film 8\n",
      "films 0\n",
      "final 0\n",
      "friend 0\n",
      "fun 0\n",
      "funny 0\n",
      "game 0\n",
      "gets 0\n",
      "getting 0\n",
      "given 0\n",
      "gives 0\n",
      "goes 1\n",
      "going 1\n",
      "good 1\n",
      "got 0\n",
      "great 0\n",
      "group 0\n",
      "guy 0\n",
      "half 0\n",
      "hand 0\n",
      "hard 0\n",
      "having 0\n",
      "head 0\n",
      "help 0\n",
      "high 0\n",
      "hollywood 0\n",
      "home 0\n",
      "horror 0\n",
      "hour 0\n",
      "human 0\n",
      "humor 0\n",
      "idea 0\n",
      "instead 1\n",
      "interesting 0\n",
      "isn 0\n",
      "jackie 0\n",
      "james 0\n",
      "job 0\n",
      "john 0\n",
      "just 0\n",
      "kind 2\n",
      "know 0\n",
      "left 0\n",
      "let 0\n",
      "life 1\n",
      "like 0\n",
      "line 0\n",
      "little 0\n",
      "ll 0\n",
      "long 0\n",
      "look 0\n",
      "looking 0\n",
      "looks 0\n",
      "lost 1\n",
      "lot 0\n",
      "love 0\n",
      "main 0\n",
      "make 0\n",
      "makes 0\n",
      "making 1\n",
      "man 0\n",
      "men 1\n",
      "michael 0\n",
      "mind 0\n",
      "minutes 0\n",
      "moments 0\n",
      "money 0\n",
      "mother 0\n",
      "movie 4\n",
      "movies 0\n",
      "mr 0\n",
      "music 0\n",
      "new 0\n",
      "night 0\n",
      "old 1\n",
      "original 0\n",
      "people 0\n",
      "performance 0\n",
      "performances 0\n",
      "picture 0\n",
      "place 0\n",
      "planet 1\n",
      "play 0\n",
      "played 0\n",
      "plays 0\n",
      "plot 1\n",
      "point 1\n",
      "pretty 0\n",
      "probably 0\n",
      "quite 0\n",
      "real 0\n",
      "really 0\n",
      "reason 0\n",
      "rest 2\n",
      "right 2\n",
      "role 0\n",
      "run 0\n",
      "say 1\n",
      "scene 0\n",
      "scenes 0\n",
      "school 0\n",
      "screen 0\n",
      "script 0\n",
      "second 0\n",
      "seen 0\n",
      "sense 0\n",
      "series 0\n",
      "set 0\n",
      "sex 0\n",
      "shows 0\n",
      "simply 0\n",
      "small 0\n",
      "soon 0\n",
      "special 0\n",
      "star 1\n",
      "stars 0\n",
      "story 0\n",
      "sure 0\n",
      "takes 1\n",
      "thing 0\n",
      "things 0\n",
      "think 1\n",
      "thought 0\n",
      "time 3\n",
      "times 0\n",
      "town 0\n",
      "true 0\n",
      "trying 0\n",
      "turns 0\n",
      "unfortunately 0\n",
      "use 0\n",
      "ve 0\n",
      "want 0\n",
      "wants 0\n",
      "watch 0\n",
      "watching 0\n",
      "way 1\n",
      "wife 2\n",
      "woman 0\n",
      "won 0\n",
      "work 0\n",
      "works 0\n",
      "world 0\n",
      "written 0\n",
      "wrong 0\n",
      "year 1\n",
      "years 0\n",
      "young 0\n"
     ]
    }
   ],
   "source": [
    "bad_review_wordcount = bad_review_data_features.toarray()[0]\n",
    "for i in range(0, len(bad_review_wordcount)):\n",
    "    print(vectorizer.get_feature_names()[i], bad_review_wordcount[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may notice there aren't a lot of matches here, a lot of zeros.  This isn't uncommon.  In addition, many of the terms, like \"film\", are too general to be associated with a positive or negative review.  To use this in a real training scenario, we'd want to increase the number of words in the bag considerably.  When we apply this in the next example, where we apply machine learning techniques to predict whether a review is positive or negative, we'll go up to 5,000 terms.  You can do that by changing a single parameter in some of the method calls - feel free to do that now before moving on to the next section.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
